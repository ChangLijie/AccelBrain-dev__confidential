version: '3.8'

networks:
  accelbrain_network:
    driver: bridge
    ipam:  
      driver: default
      config:
        - subnet: "192.168.1.0/24" 

services:
  nginx:
    image: nginx:latest
    container_name: nginx_reverse_proxy
    volumes:
      - ./setting/nginx.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "80:80" 
    networks:
      accelbrain_network:
        ipv4_address: 192.168.1.10
    depends_on:
      - model_handler
      - ollama
      - open_webui
    restart: unless-stopped

  model_handler:
    image: innodiskorg/model_handler:v0.1
    container_name: model_handler
    working_dir: /workspace
    environment:
      - MODEL_SERVER_IP=model_server
      - MODEL_SERVER_PORT=11434
    volumes:
      - ./model_handler/log:/workspace/log
      - ./models:/workspace/models 
    networks:
      accelbrain_network:
        ipv4_address: 192.168.1.20  
    ports:
      - "5000:5000" 
    restart: unless-stopped  

  ollama:
    image: innodiskorg/ollama:v0.1
    container_name: model_server
    runtime: nvidia 
    environment:
      - OLLAMA_MAX_LOADED_MODELS=2
    volumes:
      - ./models/ollama:/root/.ollama  
      - ./models/inno:/home      
    networks:
      accelbrain_network:
        ipv4_address: 192.168.1.30  
    ports:
      - "11434:11434"  
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]  
    restart: unless-stopped 

  open_webui:
    image: innodiskorg/open-webui
    container_name: open_webui
    environment:
      - MODEL_HANDLER_IP=model_handler
      - MODEL_HANDLER_PORT=5000
      - OLLAMA_BASE_URL=http://model_server:11434
    volumes:
      - ./openwebui/backend:/app/backend
    networks:
      accelbrain_network:
        ipv4_address: 192.168.1.40  # open_webui 的固定 IP
    ports:
      - "3000:8080"
    depends_on:
      - ollama
      - model_handler
    restart: unless-stopped
