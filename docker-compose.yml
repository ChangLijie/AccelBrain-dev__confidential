version: '3.8'

networks:
  accelbrain_network:
    driver: bridge
    ipam:  
      driver: default
      config:
        - subnet: "192.168.1.0/24" 

services:
  nginx:
    image: nginx:latest
    container_name: nginx_reverse_proxy
    volumes:
      - ./setting/nginx.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "6589:80" 
    networks:
      accelbrain_network:
        ipv4_address: 192.168.1.10
    depends_on:
      - model_handler
      - model_server
      - chatbot
    restart: unless-stopped

  model_handler:
    image: innodiskorg/model_handler:v0.1.1
    container_name: model_handler
    working_dir: /workspace
    environment:
      - MODEL_SERVER_IP=model_server
      - MODEL_SERVER_PORT=11434
    volumes:
      - ./model_handler/log:/workspace/log
      - ./models:/workspace/models 
    networks:
      accelbrain_network:
        ipv4_address: 192.168.1.20  
    restart: unless-stopped  

  model_server:
    image: innodiskorg/ollama:v0.1
    container_name: model_server
    runtime: nvidia 
    environment:
      - OLLAMA_MAX_LOADED_MODELS=3
    volumes:
      - ./models/ollama:/root/.ollama  
      - ./models/inno:/home      
    networks:
      accelbrain_network:
        ipv4_address: 192.168.1.30  
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]  
    restart: unless-stopped 

  chatbot:
    image: innodiskorg/open-webui:v0.3
    container_name: chatbot
    environment:
      - MODEL_HANDLER_IP=model_handler
      - MODEL_HANDLER_PORT=5000
      - OLLAMA_BASE_URL=http://model_server:11434
    volumes:
      - ./chatbot/data:/app/backend/data
    networks:
      accelbrain_network:
        ipv4_address: 192.168.1.40  # 
    ports:
      - "3000:8080" 
    depends_on:
      - model_server
      - model_handler
    restart: unless-stopped
